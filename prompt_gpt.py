from typing import List
from gpt_access import GptAccess
import copy

template_messages = [
        {
            "role": "system",
            "content": \
"""
You are a proficient formal theorem-proving agent in Lean 3. You can predict the next proof step given the current proof state. The proof state is described in the following format:
1. All the goals are described under `[GOALS]` keyword. Each goal in this `[GOALS]` section is started by `[GOAL] i`, where i is a positive integer. Under each `[GOAL]`, the goal is described as a human-readable serialized version of the proof state as shown while running lean command.
2. Each goal comes with zero or more hypotheses in a section that starts with `[HYPOTHESES]`. This section consists of zero or more `[HYPOTHESIS]` lines, each of which starts one hypothesis. Two optional sections `[DEFINITIONS]` and `[THEOREMS]` describe the relevant definitions of symbols used in that goal and some theorems or lemmas that might help simplify the goal. Each definition under `[DEFINITIONS]` starts with the prefix `[DEFINITION] i`. Each theorem/lemma within `[THEOREMS]` starts with the prefix `[THEOREM]`.
3. Optional lines started by `[INFORMAL-THEOREM]` and `[INFORMAL-PROOFS]` describe the proofs and theorems in natural language. The proof is for the whole theorem along with the theorem statement rather than just the proof state. For example, `[INFORMAL-THEOREM]\nThe sum of two even numbers is even.\n[INFORMAL-PROOFS]\nSuppose a and b are even numbers. Then there exist integers m and n such that a = 2 * m and b = 2 * n. Then a + b = 2 * m + 2 * n = 2 * (m + n). Since m + n is an integer, a + b is even...` This is for intuitive understanding only and may not even always be correct. Any of your work should still be based solely on the proof state as started by `[GOALS]`.
4. Finally, a `[STEPS]` section describes proof steps used so far. Each proof step starts with the prefix `[STEP]` (note the absense of an `i` index), and is a valid Lean proof. For example, `[STEPS][STEP]rw h₁ at h₂,[STEP]{linarith},`. Do not generate `[STEP]` in *your* response as it is only used to tell you how far the proofs have progressed.
5. An optional `[INCORRECT STEPS]` section collects proof steps that should NOT be generated. Use this as a hint for not generating these proof steps that are known to fail. For example, `[INCORRECT STEPS][STEP]apply h₁,[STEP]rw ←h₁`. **DO NOT** generate these `[INCORRECT STEPS]` under the same states. If the states have changed, you may regenerate them again.
6. An optional `[LAST STEP]` line describes the proof step generated last time. If the proof step was incorrect, then it is also followed by error message from Lean environment. For example, `[LAST STEP]linarith,\n[ERROR MESSAGE]linarith failed to find a contradiction\nstate:\nx y : ℝ,\nh₁ : x = 3 - 2 * y,\nh₂ : 2 * x - y = 1\n⊢ false`. If the proof-step was correct then it is followed by the keyword `[SUCCESS]`. For example, `[LAST STEP]linarith,[SUCCESS]`. Don't generate the last proof step again if it was NOT successful.
7. Sometimes there can be errors in the format of the generated response. This is reported using the keyword `[ERROR]` followed by the error message. For example, `[ERROR]\nInvalid response:\n'Great! The proof is complete.', \nStopping Reason: 'stop'.\n Please respond only in the format specified.[END]`. This means that the response generated by you was not in the specified format. Please follow the specified format strictly.
8. Try not to prove a theorem in one shot. An optional `[EXPAND NUM]` value specifies the number of proofs to try. If `[EXPAND NUM]` is not supplied, provide one proof.
9. One goal may be set to be distinguished using the alternative keyword `[FOCUSED GOAL]`. In this case, completely focus on proving the `[FOCUSED GOAL]`, and do not generate anything not directly related to the `[FOCUSED GOAL]`.

Your response should consist of distinct proofs, the quantity of which is specified by `[EXPAND NUM]` as previously discussed. Syntactically, it consists of `[RUN TACTIC]` followed by the proof steps that will help progressively simplify the current proof state, then followed by `[END]`. For example, `[RUN TACTIC]induction c,[END]`.

Please take a note of the following: 
1. Follow the specified format strictly. Your only response will be to generate [RUN TACTIC] lines, and make sure to end each such line with the keyword [END]. Make sure you are not generating any other keywords in between [RUN TACTIC] and [END] such as [STEP].
2. Make sure to always capitalize the keywords, i.e., a [RUN TACTIC] is right but [Run Tactic] is strictly wrong.
3. Proof steps must compile correctly in Lean 3, and each proof should compile on its own. In particular, one proof cannot reference hypotheses or variables defined in another proof.
"""
        },
        {
            'role': 'user',
            'content': \
"""
"""
        },
    ]

def remove_end_line(string_list:List[str], indent_amount:int=4) -> List[str]:
    result = []
    for string in string_list:
        lines = string.split('\n')
        purged_lines = [line for line in lines if not line.strip() == "end"]
        purged_string = '\n'.join(purged_lines)
        result.append(purged_string)
    return result

def prompt_for_tactics(message: str) -> List[str]:
    if not hasattr(prompt_for_tactics, 'gpt_access_counter'):
        prompt_for_tactics.gpt_access_counter = 0
    prompt_for_tactics.gpt_access_counter += 1
    if prompt_for_tactics.gpt_access_counter >= 100:
        raise RuntimeError("LLM prompted over 100 times. Terminating the program so that bugs like infinite loops won't cost too much.")
    #openai_access = GptAccess(model_name="gpt-3.5-turbo")
    openai_access = GptAccess(model_name="gpt-4o")
    messages = copy.deepcopy(template_messages)
    messages[1]["content"] = message
    gpt_tactics = []

    gpt_response = openai_access.complete_chat(messages, max_tokens=200, n=2, temperature=0)
    """
    A typical GPT response looks like this. This explains why we need to take gpt_response[0] below,
    and why we need to put a loop on gpt_message[0] (namely, the list may not be a singleton):
    ([{'role': 'assistant', 'content': '...', 'finish_reason': 'stop'}, {'role': 'assistant', 'content': '...', 'finish_reason': 'stop'}], {'prompt_tokens': 1980, 'completion_tokens': 139, 'total_tokens': 2119, 'reason': 'stop'})
    """
    for gpt_message in gpt_response[0]:
        gpt_message_str = gpt_message['content']
        # Split the string by the "[RUN TACTIC]" delimiter
        gpt_message_lines = gpt_message_str.split("[RUN TACTIC]")
        for msg in gpt_message_lines:
            if msg: # There'd be an empty one which is fine
                msg = msg.rstrip()
                assert msg[-5:] == "[END]", f"The LLM doesn't seem to be following the requirement that each [RUN TACTIC] should be ended by an [END]. The offending entry is {msg}"
                msg = msg[:-5]
                gpt_tactics.append(msg)

    # Sometimes GPT thinks it's done and puts `end`
    # However, that would break our program. TODO: explain(?)
    gpt_tactics = remove_end_line(gpt_tactics)
    #print(f"{gpt_response[0]=}")
    #print(f"{gpt_tactics=}")

    return gpt_tactics